{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b8d4b4-e4a2-49e8-84c6-91f3688873a7",
   "metadata": {},
   "source": [
    "# Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c080b2b-4133-43f6-af04-94ef00932787",
   "metadata": {},
   "source": [
    "## Assignment 10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba311d9-968c-47f0-bcc9-7b62e007c579",
   "metadata": {},
   "source": [
    "What is a first or n$^{\\text{th}}$ order Markov model? How can it be learned?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d26efb-97c2-43db-8915-36a23dea4632",
   "metadata": {},
   "source": [
    "Markov models are stochastic models describing a set of states and associated probabilities of transitioning between these states (including the initialisation probabilities). Each state is parametrised differently and may be associatd with emission probabilities with regards to explicitly observable states. The model posits only one state is \"active\" at any one time. \n",
    "\n",
    "The central idea of a Markov model is a limited \"memory\" of the history of all time points and the associated states. The range of states retained (the cardinality) is limited by the order of the Markov model. The conditional transition probabilities between states in a first order Markov model depend solely on which state preceded the current time point, second order Markov model considers the two previous states, and so on.\n",
    "\n",
    "The probability of the current state $X$ (a random variable) being in one state $A$ in a first-order Markov model given we were in state $B$ beforehand is given by:\n",
    "\n",
    "$$\n",
    "p(X_i=A|X_{i-1}=B)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f2e34-cdbc-4830-99e0-30941975a6ef",
   "metadata": {},
   "source": [
    "How we learn a Markov model may depend on the observarbility of the states (fully observable vs. partially observable) and whether there are actions that an agent can take (a chain vs a decision process). \n",
    "\n",
    "For a Markov chain, where everything is observable, we might simply count the transitions to fully describe the transition matrix as well as the initial probabilities. Similarly, a MDP can be learned from what is essentially a closed form solution, such as Q-learning, although we might need to use an iterative algorithm for optimal policy learning. \n",
    "\n",
    "For the partially unobservable Markov models, HMM or POMDP, there is no closed form solution. In that case, an iterative algorithm which maximises the likelihood is a good example, such as EM or MCMC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680deb22-2342-4000-80c3-4b5183594f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422a4e5-328c-40e1-88e6-8f5b1512fe86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86366228-0cb8-4d80-8863-8310f85d60b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
