{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ce6532-22fa-4129-87ea-cd462123e663",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7abdd38-7cfc-4d24-9487-18247c82b504",
   "metadata": {},
   "source": [
    "## Assignment 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3cd99-92d1-4b1c-9bf2-69644efd3d70",
   "metadata": {},
   "source": [
    "Given are the following points A(−0.5;1), B(−1;−1.5), C(−1.5;1.5), D(1.5;−0.5) and E(0.5;−0.5) as shown below. The goal is to learn a decision function $f$ using Linear Learning Machines (LLM) that separates A and E from the other points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f2d6a-ddd3-4adc-bac1-634858b26b5b",
   "metadata": {},
   "source": [
    "<img src=\"5_figure.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb9fae-3ca7-439a-b0ab-6a5e030b23fa",
   "metadata": {},
   "source": [
    "a) Compute the kernel matrix for $K(x_1, x_2) = \\langle x_1, x_2  \\rangle ^2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b2f28-8925-4357-be69-c2e79b5a4905",
   "metadata": {},
   "source": [
    "In this section, we need to battle not only the exercise but also very unclear notation. With regards to the kernel matrix, what we are asked to do is to compute the square of the dot product between the vectors $x_i$ which are made up of 2 values, confusingly called $x$ and $y$ in the figure. For the purposes of clarity, I will refrain from using $x$ to refer to the abscissa feature and instead $x_i$ will signify the entire vector made up of 2 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdd3421b-dbe5-492b-8ffe-9d65fe0620fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5625,  1.    ,  5.0625,  1.5625,  0.5625],\n",
       "       [ 1.    , 10.5625,  0.5625,  0.5625,  0.0625],\n",
       "       [ 5.0625,  0.5625, 20.25  ,  9.    ,  2.25  ],\n",
       "       [ 1.5625,  0.5625,  9.    ,  6.25  ,  1.    ],\n",
       "       [ 0.5625,  0.0625,  2.25  ,  1.    ,  0.25  ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pre-allocate an empty array \n",
    "K = np.empty((5, 5))\n",
    "\n",
    "# Initialise all data points \n",
    "A = [-0.5, 1, 1]; B = [-1, -1.5, -1]; C = [-1.5, 1.5, -1]; D = [1.5, -0.5, -1]; E = [0.5, -0.5, 1]\n",
    "data = [A, B, C, D, E]\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        K[i, j] = np.dot(data[i][:2], data[j][:2])**2\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7f996-fa39-4ea3-a26f-b5a33001c219",
   "metadata": {},
   "source": [
    "b) Apply the perceptron update rule in dual representation (see below) to all five\n",
    "data points using the kernel from (a). Start with α = $\\vec{0}$, $b = 0$ and repeat the\n",
    "updating until all data points can be correctly classified:\n",
    "\n",
    "$$\n",
    "\\forall i : y_i \\left( \\sum_{j=1}^n \\alpha_j y_j \\langle x_1, x_2  \\rangle ^2 + b \\right) \\leq 0  \\Rightarrow \\alpha_i = \\alpha_i + 1; b = b + y_i (\\underset{j}{\\max} || x_j ||)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b1ab0-e91d-47c1-8ddf-420f1e4be24b",
   "metadata": {},
   "source": [
    "Here is more bad notation, to the extent the notation becomes atrocious. Firstly, while not necessarily confusing, it is not explained that the $i$ and $j$ indices correspond to the same loop executed twice, we are doing a pair-wise operation in other words. Secondly, the dot product, indicated by the angled brackets, actually refers to the dot product between $x_i$ and $x_j$. Lastly, the change in $b$ is particularly poorly given because it's a single value that does not change from one iteration over $i$ to another, it's always going to be the squared norm of point C (4.5) multiplied by some factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e22ea2d-f252-4360-a79f-6da53953a3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decision value 0.0 is below (or equal) 0, so new alpha values are [1 0 0 0 0] and b is 4.5\n",
      "The decision value -5.5 is below (or equal) 0, so new alpha values are [1 1 0 0 0] and b is 0.0\n",
      "The decision value -4.5 is below (or equal) 0, so new alpha values are [1 1 1 0 0] and b is -4.5\n",
      "The decision value -6.25 is below (or equal) 0, so new alpha values are [1 1 1 0 1] and b is 0.0\n",
      "The decision value -3.9375 is below (or equal) 0, so new alpha values are [2 1 1 0 1] and b is 4.5\n"
     ]
    }
   ],
   "source": [
    "# Initialise the Lagrangian multiplier vector\n",
    "lagrange_vec = np.array([0]*5)\n",
    "\n",
    "# Initialise b\n",
    "b = 0\n",
    "\n",
    "# Create a flag\n",
    "miss_clas = True \n",
    "\n",
    "while miss_clas:\n",
    "    miss_clas = False\n",
    "    \n",
    "    for i in range(5):\n",
    "        clsf = data[i][-1] * (sum([lagrange_vec[j]*data[j][-1]*K[i, j] for j in range(5)]) + b)\n",
    "    \n",
    "        if clsf <= 0:\n",
    "            lagrange_vec[i] += 1\n",
    "            b += 4.5 * data[i][-1]\n",
    "            miss_clas = True \n",
    "            \n",
    "            print(f\"The decision value {clsf} is below (or equal to) 0, so new alpha values are {lagrange_vec} and b is {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b99fe-81e7-4b8f-931a-05ef08046b03",
   "metadata": {},
   "source": [
    "c) Classify the point X(-1;0) using the learned hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d5e5ac63-2719-4675-947e-8beeade3c8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new point is classified as a cross (1) since its decision values is 2.0 which is larger than zero\n"
     ]
    }
   ],
   "source": [
    "clsf_np = data[i][-1] * (sum([lagrange_vec[j]*data[j][-1]*np.dot(data[j][:2], [-1, 0])** 2 for j in range(5)]) + b)\n",
    "print(f\"The new point is classified as a cross (1) since its decision values is {clsf_np} which is larger than zero\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412fe5f8-b0a1-46b1-9a6a-4ea564991829",
   "metadata": {},
   "source": [
    "## Assignment 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eabc408-ee7b-4210-ba4c-1504c2e786c1",
   "metadata": {},
   "source": [
    "What is the main idea behind linear Support Vector Machines (SVMs)? Illustrate your\n",
    "explanation by drawing a figure. What equations are used in SVMs? How can a separating\n",
    "hyperplane with a maximal margin be found?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88011e67-a97c-47b5-8c96-b4de90d69365",
   "metadata": {},
   "source": [
    "The basic idea of SVMs is to find a hyperplane by virtue of implicitly mapping data to a space where they can be linearly separated using kernels. An important property of SVMs is that the determination of the model parameters corresponds to a convex optimisation problem, thus rendering any local solution a global optimum. \n",
    "\n",
    "The central equation of SVMs is centred around the decision function $f(x) = \\langle w, x \\rangle + b = \\sum_i \\alpha_i y_i \\langle x_i, x \\rangle+b$\n",
    "\n",
    "Below is a figure illustrating the implicit mapping to a new space with the associated kernel, newly rendering the problem linearly separable\n",
    "\n",
    "\n",
    "<img src=\"5_2_figure.png\" width=\"600\">\n",
    "\n",
    "\n",
    "In order to find the hyperplane with the maximal margin, we seek to minimise the size of the vector $w$ as per the figure below. Note the inconsistent notation with the bias term being subtracted rather than added, though the meaning isn't changed as the unary negation doesn't affect the linearity of the separation. \n",
    "\n",
    "<img src=\"5_3_figure.png\" width=\"400\">\n",
    "\n",
    "The minimisation itself is done in the context of constrained optimisation using Lagrange multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e8d2f-31a9-488f-bc59-7fafee0f352d",
   "metadata": {},
   "source": [
    "## Assignment 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad79f5-7e68-4a4e-a19d-8b3cd4be4ad7",
   "metadata": {},
   "source": [
    "How is the optimization problem of a Support Vector Machine modified to handle not\n",
    "linearly separable data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0bda57-c188-4923-bab7-447228bd5f77",
   "metadata": {},
   "source": [
    "The central idea of SVMs is to introduce kernels which implicitly map the data into a new space where they are linearly separable. We can also use slack variables for a soft margin which allows for misclassification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0c4812-f38f-4cd6-9f8e-226dff930bf7",
   "metadata": {},
   "source": [
    "## Assignment 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b4215-26e6-4c42-abfa-a443d33b45c9",
   "metadata": {},
   "source": [
    "Present a kernel that was not described in the course. You should be able to describe how it\n",
    "is computed and in which scenarios it can be used. Look for articles describing this kernel.\n",
    "Examples of such kernels are string, tree or graph kernels or kernels for bioinformatics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08e103-7123-4ea4-96e6-5b4a7b7967a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
